{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "### Predictive Lead Scoring\n",
        "\n",
        "dataset ini berisi 41.188 baris, 21 kolom\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   UCI ML Repository : https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
        "2. **Nama Dataset**:  \n",
        "   bank-full.csv\n",
        "3. **Tujuan**:  \n",
        "   memprediksi kesuksesan telemarketing\n",
        "4. **Format Dataset**:  \n",
        "   CSV\n",
        "5. **Informasi Dataset**: \n",
        "   \n",
        "A. Data Nasabah (Bank Client Data)\n",
        "Informasi dasar dan demografis nasabah.\n",
        "\n",
        "- age: Umur nasabah (Numerik).\n",
        "\n",
        "- job: Jenis pekerjaan nasabah (Kategorikal: \"admin.\", \"blue-collar\", \"technician\", \"unknown\", dll).\n",
        "\n",
        "- marital: Status pernikahan (Kategorikal: \"married\", \"single\", \"divorced\", \"unknown\").\n",
        "\n",
        "- education: Tingkat pendidikan (Kategorikal: \"basic.4y\", \"high.school\", \"university.degree\", \"unknown\", dll).\n",
        "\n",
        "- default: Apakah nasabah memiliki kredit macet? (Kategorikal: \"no\", \"yes\", \"unknown\").\n",
        "\n",
        "- housing: Apakah nasabah memiliki pinjaman perumahan? (Kategorikal: \"no\", \"yes\", \"unknown\").\n",
        "\n",
        "- loan: Apakah nasabah memiliki pinjaman pribadi? (Kategorikal: \"no\", \"yes\", \"unknown\").\n",
        "\n",
        "B. Data Terkait Kontak Terakhir (Current Campaign)\n",
        "Informasi tentang kontak yang dilakukan di kampanye yang sedang berjalan ini.\n",
        "\n",
        "- contact: Tipe komunikasi yang digunakan (Kategorikal: \"cellular\", \"telephone\").\n",
        "\n",
        "- month: Bulan terakhir kali nasabah dikontak (Kategorikal: \"jan\", \"feb\", ..., \"dec\").\n",
        "\n",
        "- day_of_week: Hari terakhir kali nasabah dikontak (Kategorikal: \"mon\", \"tue\", \"wed\", \"thu\", \"fri\").\n",
        "\n",
        "- duration: Durasi telepon pada kontak terakhir (Numerik, dalam detik).\n",
        "\n",
        "* Catatan Penting: fitur ini (duration) harus dibuang (drop) karena menyebabkan kebocoran data (data leakage), sebab durasi baru diketahui setelah telepon selesai.\n",
        "\n",
        "C. Atribut Lainnya (Other Attributes)\n",
        "Informasi histori kampanye sebelumnya dan konteks ekonomi makro.\n",
        "\n",
        "- campaign: Jumlah kontak yang dilakukan untuk nasabah ini selama kampanye ini (Numerik, termasuk kontak terakhir).\n",
        "\n",
        "- pdays: Jumlah hari yang berlalu sejak nasabah terakhir dikontak dari kampanye sebelumnya (Numerik; 999 berarti \"belum pernah dikontak sebelumnya\").\n",
        "\n",
        "- previous: Jumlah kontak yang dilakukan untuk nasabah ini sebelum kampanye ini (Numerik).\n",
        "\n",
        "- poutcome: Hasil dari kampanye pemasaran sebelumnya (Kategorikal: \"failure\", \"nonexistent\", \"success\").\n",
        "\n",
        "D. Konteks Sosial & Ekonomi (Social & Economic Context)\n",
        "Indikator ekonomi makro saat kontak terakhir dilakukan.\n",
        "\n",
        "- emp.var.rate: Tingkat variasi pekerjaan (Indikator ekonomi triwulanan).\n",
        "\n",
        "- cons.price.idx: Indeks harga konsumen (Indikator ekonomi bulanan).\n",
        "\n",
        "- cons.conf.idx: Indeks kepercayaan konsumen (Indikator ekonomi bulanan).\n",
        "\n",
        "- euribor3m: Suku bunga Euribor 3 bulan (Indikator ekonomi harian).\n",
        "\n",
        "- nr.employed: Jumlah karyawan (Indikator ekonomi triwulanan).\n",
        "\n",
        "E. Variabel Target (Output Variable)\n",
        "Hasil akhir yang ingin kita prediksi.\n",
        "\n",
        "- y: Target kita. Apakah nasabah pada akhirnya berlangganan deposito berjangka? (Binary: \"yes\" atau \"no\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
        "\n",
        "# Set Data Display options\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3ItwTen_7E"
      },
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "# Simpel & Bersih: Load langsung dari path yang ditemukan\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path yang berhasil ditemukan (Colab)\n",
        "file_path = 'bank-additional/bank-additional-full.csv'\n",
        "\n",
        "# Fallback ke path lokal jika tidak ada\n",
        "if not os.path.exists(file_path):\n",
        "    file_path = '../dataset/bank-additional-full.csv'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"Dataset tidak ditemukan. Pastikan Anda sudah menjalankan sel download di atas atau path sudah benar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# Check basic info\n",
        "print(\"Info Dataset:\")\n",
        "df.info()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize Target Distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='y', data=df)\n",
        "plt.title('Distribution of Target Variable (y)')\n",
        "plt.show()\n",
        "\n",
        "# Visualize Numerical Distributions\n",
        "df.hist(bins=20, figsize=(20, 15))\n",
        "plt.show()\n",
        "\n",
        "# === Visualisasi Data Kategorikal (Bar Charts) ===\n",
        "print(\"\\nVisualisasi Data Kategorikal (Bar Charts):\")\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "for col in categorical_cols:\n",
        "    if col != 'y':\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.countplot(y=col, data=df, order=df[col].value_counts().index, palette='viridis')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.show()\n",
        "\n",
        "# === Visualisasi Korelasi (Heatmap) ===\n",
        "print(\"\\nVisualisasi Korelasi (Heatmap):\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf8KUPXLg5r"
      },
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
        "\n",
        "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# === 1. Feature Engineering ===\n",
        "\n",
        "# Binning 'age'\n",
        "age_bins = [0, 29, 39, 49, 59, 100]\n",
        "age_labels = ['<30', '30-39', '40-49', '50-59', '60+']\n",
        "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "# Konsolidasi 'education'\n",
        "df['education'] = df['education'].replace(['basic.4y', 'basic.6y', 'basic.9y'], 'basic')\n",
        "\n",
        "# Transformasi 'pdays'\n",
        "df['pernah_kontak'] = df['pdays'].apply(lambda x: 'no' if x == 999 else 'yes')\n",
        "\n",
        "# Fitur Rasio\n",
        "total_contacts = df['previous'] + df['campaign']\n",
        "df['previous_ratio'] = df['previous'] / total_contacts\n",
        "df['previous_ratio'] = df['previous_ratio'].fillna(0)\n",
        "\n",
        "# Drop duration (Data Leakage)\n",
        "if 'duration' in df.columns:\n",
        "    df = df.drop(columns=['duration'])\n",
        "\n",
        "# Handle Duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# === 2. Encoding Target ===\n",
        "le = LabelEncoder()\n",
        "df['y'] = le.fit_transform(df['y'])\n",
        "\n",
        "# === 3. Split Data (Train/Test) FIRST ===\n",
        "# Penting: Split dulu baru Preprocessing untuk mencegah Data Leakage\n",
        "X = df.drop(['y', 'age', 'pdays'], axis=1)\n",
        "y = df['y']\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Raw Train shape: {X_train_raw.shape}\")\n",
        "print(f\"Raw Test shape: {X_test_raw.shape}\")\n",
        "\n",
        "# === 4. Define Transformers ===\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# ColumnTransformer: Scale Numerals AND Encode Categoricals\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# === 5. Pipeline & Transformation ===\n",
        "# Fit only on Train, Transform both\n",
        "print(\"Preprocessing data...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train_raw)\n",
        "X_test_processed = preprocessor.transform(X_test_raw)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Convert back to DataFrame (for SMOTE clarity)\n",
        "if hasattr(X_train_processed, 'toarray'):\n",
        "   X_train_processed = X_train_processed.toarray()\n",
        "   X_test_processed = X_test_processed.toarray()\n",
        "   \n",
        "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
        "\n",
        "# === 6. SMOTE (Oversampling) ===\n",
        "# Terapkan SMOTE hanya pada Data Training!\n",
        "print(\"Applying SMOTE to Training Data...\")\n",
        "print(f\"Before SMOTE: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_df, y_train)\n",
        "\n",
        "print(f\"After SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n",
        "\n",
        "# === 7. Saving Processed Data ===\n",
        "if not os.path.exists('output'):\n",
        "    os.makedirs('output')\n",
        "\n",
        "# A. Data Latih (SMOTE)\n",
        "train_export = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_names), \n",
        "                          pd.Series(y_train_resampled, name='y')], axis=1)\n",
        "\n",
        "# B. Data Uji (Original)\n",
        "test_export = pd.concat([X_test_df, \n",
        "                         pd.Series(y_test.values, name='y')], axis=1)\n",
        "\n",
        "# C. Data Full Processed (Tanpa SMOTE - Gabungan Train+Test)\n",
        "# Hati-hati: Kita gabungkan X_train_df (bukan resampled) dan X_test_df\n",
        "full_X = pd.concat([X_train_df, X_test_df], ignore_index=True)\n",
        "full_y = pd.concat([pd.Series(y_train.values), pd.Series(y_test.values)], ignore_index=True)\n",
        "full_export = pd.concat([full_X, full_y.rename('y')], axis=1)\n",
        "\n",
        "train_export.to_csv('output/train.csv', index=False)\n",
        "test_export.to_csv('output/test.csv', index=False)\n",
        "full_export.to_csv('output/data_processed.csv', index=False)\n",
        "\n",
        "print(f\"Saved train.csv (SMOTE): {train_export.shape}\")\n",
        "print(f\"Saved test.csv: {test_export.shape}\")\n",
        "print(f\"Saved data_processed.csv (All): {full_export.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
